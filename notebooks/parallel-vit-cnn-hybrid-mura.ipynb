{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1172563b-3b8a-4bf2-a934-4ee803fe44a0",
   "metadata": {},
   "source": [
    "# Parallel ViT-CNN Hybrid with MURA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98de89e-9b55-411d-8ad6-c8ebeb866388",
   "metadata": {},
   "source": [
    "## GPU Availability Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d5456-2590-496c-92d8-da9155b6bd84",
   "metadata": {},
   "source": [
    "We start with GPU availability check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00846f99-0a56-4981-92f2-925fd7204f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.20.0\n",
      "TF GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "PyTourch Version: 2.10.0+cu128\n",
      "PyTourch GPU Available: True\n",
      "Device Name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print(f\"TF GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "print(f\"\\nPyTourch Version: {torch.__version__}\")\n",
    "print(f\"PyTourch GPU Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219151f8-11a9-4e6b-987e-58f6917d0514",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f9741-616a-42ee-a4ac-00675d60411a",
   "metadata": {},
   "source": [
    "We define the MURA dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b90fadb-4d7a-442f-8e82-9a60c3be7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MURAManager:\n",
    "    def __init__(self, csv_file, sample_n=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with image paths.\n",
    "        \"\"\"\n",
    "        # Read CSV. Assuming no header based on standard MURA csv format\n",
    "        self.df = pd.read_csv(csv_file, header=None, names=['filepath'])\n",
    "\n",
    "        if sample_n:\n",
    "            self.df = self.df.sample(n=sample_n)\n",
    "        \n",
    "        # --- 1. Path Replacement ---\n",
    "        # Replace 'MURA-v1.1' with '/tf/data/mura'\n",
    "        # Old: MURA-v1.1/train/XR_SHOULDER/...\n",
    "        # New: /tf/data/mura/train/XR_SHOULDER/...\n",
    "        self.df['filepath'] = self.df['filepath'].str.replace('MURA-v1.1', '/tf/data/mura', regex=False)\n",
    "\n",
    "        # --- 2. Feature Engineering ---\n",
    "        # New Path structure: \n",
    "        # /tf/data/mura/train/XR_SHOULDER/patient00001/study1_positive/image1.png\n",
    "        # Split by '/':\n",
    "        # [0]=\"\" [1]=\"tf\" [2]=\"data\" [3]=\"mura\" [4]=\"train\" [5]=\"XR_SHOULDER\" [6]=\"patient00001\"\n",
    "        \n",
    "        # Extract Body Part (Class Label) - Index 5\n",
    "        self.df['body_part'] = self.df['filepath'].apply(lambda x: x.split('/')[5])\n",
    "        \n",
    "        # Extract Patient ID for Group Splitting - Index 6\n",
    "        self.df['patient_id'] = self.df['filepath'].apply(lambda x: x.split('/')[6])\n",
    "\n",
    "        # 3. Filter only requested classes\n",
    "        target_classes = [\n",
    "            'XR_ELBOW', 'XR_FINGER', 'XR_FOREARM', 'XR_HAND', \n",
    "            'XR_HUMERUS', 'XR_SHOULDER', 'XR_WRIST'\n",
    "        ]\n",
    "        self.df = self.df[self.df['body_part'].isin(target_classes)].copy()\n",
    "\n",
    "        # 4. Encode Labels\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.df['label'] = self.encoder.fit_transform(self.df['body_part'])\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} images.\")\n",
    "        print(\"Class Mapping:\")\n",
    "        for idx, cls in enumerate(self.encoder.classes_):\n",
    "            print(f\"{idx}: {cls}\")\n",
    "\n",
    "    def get_splits(self, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "        # First split: Train vs (Val + Test)\n",
    "        gss_main = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=42)\n",
    "        train_idx, temp_idx = next(gss_main.split(self.df, groups=self.df['patient_id']))\n",
    "\n",
    "        train_df = self.df.iloc[train_idx]\n",
    "        temp_df = self.df.iloc[temp_idx]\n",
    "\n",
    "        # Second split: Val vs Test\n",
    "        relative_val_size = val_size / (val_size + test_size)\n",
    "        gss_val = GroupShuffleSplit(n_splits=1, train_size=relative_val_size, random_state=42)\n",
    "        val_idx, test_idx = next(gss_val.split(temp_df, groups=temp_df['patient_id']))\n",
    "\n",
    "        val_df = temp_df.iloc[val_idx]\n",
    "        test_df = temp_df.iloc[test_idx]\n",
    "\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "class MURADataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            # Fallback for missing/corrupt files during dry runs\n",
    "            print(f\"Warning: Could not load {img_path}\")\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ecdd6-d81a-4c94-a230-17811ad3f57f",
   "metadata": {},
   "source": [
    ".. and define custom resize transformation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653f9042-2818-4957-a5e4-5d61c796e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class LetterboxResize:\n",
    "    def __init__(self, target_size=(224, 224)):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # 1. Resize the image such that the longest side matches target_size\n",
    "        w, h = img.size\n",
    "        max_side = max(self.target_size)\n",
    "        scale = max_side / max(w, h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        img = TF.resize(img, (new_h, new_w))\n",
    "\n",
    "        # 2. Pad the shorter side to make it square\n",
    "        delta_w = self.target_size[0] - new_w\n",
    "        delta_h = self.target_size[1] - new_h\n",
    "        padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "\n",
    "        # Use a constant background color (black or gray)\n",
    "        return TF.pad(img, padding, fill=0, padding_mode='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b36e2-9764-4583-b248-75937daa8c5c",
   "metadata": {},
   "source": [
    ".. and from such definition, we instantiate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735a4b26-65b2-46c4-94cb-bafd9cb984d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 images.\n",
      "Class Mapping:\n",
      "0: XR_ELBOW\n",
      "1: XR_FINGER\n",
      "2: XR_FOREARM\n",
      "3: XR_HAND\n",
      "4: XR_HUMERUS\n",
      "5: XR_SHOULDER\n",
      "6: XR_WRIST\n",
      "Train size: 801 | Val size: 101 | Test size: 98\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Initialize Manager\n",
    "manager = MURAManager('/tf/data/mura/train_image_paths.csv', sample_n=1000) \n",
    "train_df, val_df, test_df = manager.get_splits()\n",
    "\n",
    "# 2. Define Transforms (Reusing from original notebook)\n",
    "# Note: MURA images vary in size/aspect ratio, so LetterboxResize is useful here.\n",
    "train_transform = transforms.Compose([\n",
    "    LetterboxResize(target_size=(224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    LetterboxResize(target_size=(224, 224)), # Use Letterbox instead of simple Resize to keep aspect ratio\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Create Dataset Objects\n",
    "train_ds = MURADataset(train_df, transform=train_transform)\n",
    "val_ds = MURADataset(val_df, transform=val_test_transform)\n",
    "test_ds = MURADataset(test_df, transform=val_test_transform)\n",
    "\n",
    "# 4. Create Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)} | Test size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20566648-242f-42cd-955b-f614dab97e11",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae116f9b-e4e0-4633-869f-c8f01636225b",
   "metadata": {},
   "source": [
    "We define training step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05cc188-6e25-4899-a8c8-80e30909d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Conformer outputs: [logits_cnn, logits_trans]\n",
    "        out_cnn, out_trans = model(images)\n",
    "\n",
    "        # Combined loss as suggested in Conformer paper\n",
    "        loss = criterion(out_cnn, labels) + criterion(out_trans, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # For accuracy, we usually average the predictions or use the transformer head\n",
    "        # pengzhiliang: prediction = (out_cnn + out_trans).argmax(dim=1)\n",
    "\n",
    "        prob_cnn = torch.softmax(out_cnn, dim=1)\n",
    "        prob_trans = torch.softmax(out_trans, dim=1)\n",
    "        prediction = (prob_cnn + prob_trans).argmax(dim=1)\n",
    "        \n",
    "        correct += (prediction == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be9866-af1a-4e01-99d5-6ec525fa605d",
   "metadata": {},
   "source": [
    ".. and validation step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ab4d9e-23db-4b1a-955f-00d6a87f5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm \n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            out_cnn, out_trans = model(images)\n",
    "            loss = criterion(out_cnn, labels) + criterion(out_trans, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "           \n",
    "            # pengzhiliang: prediction = (out_cnn + out_trans).argmax(dim=1)\n",
    "\n",
    "            prob_cnn = torch.softmax(out_cnn, dim=1)\n",
    "            prob_trans = torch.softmax(out_trans, dim=1)\n",
    "            prediction = (prob_cnn + prob_trans).argmax(dim=1)\n",
    "            \n",
    "            correct += (prediction == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52448c6-3466-4dce-b605-10d636282668",
   "metadata": {},
   "source": [
    ".. silence warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b148b9-de19-4479-abca-d115439d8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36df595-6631-48c7-ad26-4a7f55119c63",
   "metadata": {},
   "source": [
    ".. and perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c38769-74df-4b85-82f6-5054627b2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "  Train Loss: 3.3026 | Acc: 37.58%\n",
      "  Val Loss:   3.2024 | Acc: 48.51%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 3.0207 | Acc: 47.32%\n",
      "  Val Loss:   3.0135 | Acc: 50.50%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Train Loss: 2.8554 | Acc: 51.81%\n",
      "  Val Loss:   2.7368 | Acc: 56.44%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.33s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Train Loss: 2.6316 | Acc: 53.43%\n",
      "  Val Loss:   2.7485 | Acc: 53.47%\n",
      "  --> Val Loss did not improve. Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Train Loss: 2.6587 | Acc: 57.68%\n",
      "  Val Loss:   3.1600 | Acc: 49.50%\n",
      "  --> Val Loss did not improve. Early stopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:25<00:00,  3.30s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Train Loss: 2.5245 | Acc: 61.67%\n",
      "  Val Loss:   3.0636 | Acc: 55.45%\n",
      "  --> Val Loss did not improve. Early stopping counter: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "  Train Loss: 2.3477 | Acc: 66.29%\n",
      "  Val Loss:   2.6904 | Acc: 60.40%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "  Train Loss: 2.1934 | Acc: 67.29%\n",
      "  Val Loss:   2.7154 | Acc: 67.33%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss did not improve. Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "  Train Loss: 2.0709 | Acc: 72.28%\n",
      "  Val Loss:   3.2779 | Acc: 51.49%\n",
      "  --> Val Loss did not improve. Early stopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "  Train Loss: 2.0863 | Acc: 71.91%\n",
      "  Val Loss:   2.4964 | Acc: 69.31%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "  Train Loss: 1.8785 | Acc: 75.28%\n",
      "  Val Loss:   2.4195 | Acc: 64.36%\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.33s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "  Train Loss: 1.8235 | Acc: 78.28%\n",
      "  Val Loss:   2.2518 | Acc: 70.30%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "  Train Loss: 1.7949 | Acc: 76.78%\n",
      "  Val Loss:   2.3987 | Acc: 71.29%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss did not improve. Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "  Train Loss: 1.6330 | Acc: 79.03%\n",
      "  Val Loss:   1.9192 | Acc: 78.22%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "  Train Loss: 1.5528 | Acc: 80.27%\n",
      "  Val Loss:   1.7880 | Acc: 78.22%\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "  Train Loss: 1.6510 | Acc: 80.15%\n",
      "  Val Loss:   2.1237 | Acc: 72.28%\n",
      "  --> Val Loss did not improve. Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "  Train Loss: 1.5671 | Acc: 82.90%\n",
      "  Val Loss:   2.1287 | Acc: 74.26%\n",
      "  --> Val Loss did not improve. Early stopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "  Train Loss: 1.5916 | Acc: 83.52%\n",
      "  Val Loss:   2.1024 | Acc: 80.20%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss did not improve. Early stopping counter: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "  Train Loss: 1.3979 | Acc: 84.39%\n",
      "  Val Loss:   3.1612 | Acc: 69.31%\n",
      "  --> Val Loss did not improve. Early stopping counter: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:25<00:00,  3.31s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "  Train Loss: 1.3358 | Acc: 86.64%\n",
      "  Val Loss:   1.7683 | Acc: 81.19%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:26<00:00,  3.32s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "  Train Loss: 1.3081 | Acc: 85.39%\n",
      "  Val Loss:   2.0102 | Acc: 82.18%\n",
      "  --> Model Checkpointed (New Best Accuracy)!\n",
      "  --> Val Loss did not improve. Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:29<00:00,  3.45s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "  Train Loss: 1.5597 | Acc: 85.77%\n",
      "  Val Loss:   1.6608 | Acc: 82.18%\n",
      "  --> Val Loss improved. Resetting early stopping counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100% 26/26 [01:34<00:00,  3.65s/it]\n",
      "Validating: 100% 4/4 [00:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:\n",
      "  Train Loss: 1.3201 | Acc: 86.52%\n",
      "  Val Loss:   2.3559 | Acc: 76.24%\n",
      "  --> Val Loss did not improve. Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31% 8/26 [00:31<01:11,  3.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m best_val_acc = \u001b[32m0.0\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_number):\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# 1. Training & Validation steps\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Combined loss as suggested in Conformer paper\u001b[39;00m\n\u001b[32m     19\u001b[39m loss = criterion(out_cnn, labels) + criterion(out_trans, labels)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     24\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to your Conformer folder\n",
    "path_to_conformer = os.path.abspath('./Conformer')\n",
    "if path_to_conformer not in sys.path:\n",
    "    sys.path.insert(0, path_to_conformer)\n",
    "\n",
    "from models import Conformer_small_patch16\n",
    "\n",
    "# --- Hyperparameters & Setup ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conformer_small_patch16(num_classes=7).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Early Stopping Parameters ---\n",
    "epoch_number = 100          # Set a high max epochs, early stopping will handle the rest\n",
    "patience = 5               # How many epochs to wait for improvement before stopping\n",
    "min_delta = 0.001          # Minimum change to qualify as an improvement\n",
    "early_stop_counter = 0     # Internal counter\n",
    "best_val_loss = float('inf') \n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(epoch_number):\n",
    "    # 1. Training & Validation steps\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # 2. Checkpointing (Save model based on Best Accuracy)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_conformer_mura.pth')\n",
    "        print(\"  --> Model Checkpointed (New Best Accuracy)!\")\n",
    "\n",
    "    # 3. Early Stopping Logic (Based on Validation Loss)\n",
    "    if val_loss < (best_val_loss - min_delta):\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0  # Reset counter because we found a significant improvement\n",
    "        print(f\"  --> Val Loss improved. Resetting early stopping counter.\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"  --> Val Loss did not improve. Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"\\n[!] Early stopping triggered. Training stopped at epoch {epoch}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb903c-0c1b-43e9-a509-cb9b9cbe5a42",
   "metadata": {},
   "source": [
    "### Restore best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b3b0c-7a1e-4943-8821-702314910317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weights back into the model for evaluation\n",
    "model.load_state_dict(torch.load('best_conformer_mura.pth'))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Best model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2132347-b139-4032-9395-4ef30f02a6ef",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3a08d-da83-4ce9-a5f4-71d06d74a759",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741c711-b5db-451c-8569-0a0cdc28d3b3",
   "metadata": {},
   "source": [
    "We evaluate model by plotting confusion matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdb5b2-a640-4b08-b3c7-af9c094cce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Conformer dual-output handling: average the branches for prediction\n",
    "            out_cnn, out_trans = model(images)\n",
    "            outputs = (out_cnn + out_trans) / 2\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate the matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix: HAM10000')\n",
    "    plt.show()\n",
    "\n",
    "    # Also print the detailed F1-score/Recall report\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Usage:\n",
    "# Get names from the encoder we defined in the Manager class\n",
    "class_names = manager.encoder.classes_\n",
    "plot_confusion_matrix(model, test_loader, DEVICE, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1683d9c-1488-4427-a858-6868050bd848",
   "metadata": {},
   "source": [
    ".. also plot PR-curve,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb350e5-7c98-4809-804b-d6df10914195",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a187fbd-b36f-449d-955e-44697c7e3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_roc_curve(model, loader, device, class_names):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            # Handling the hybrid model output\n",
    "            out_cnn, out_trans = model(images)\n",
    "            # Use averaged probabilities\n",
    "            probs = torch.softmax((out_cnn + out_trans) / 2, dim=1)\n",
    "\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    Y_score = np.concatenate(all_probs)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Standardize label format for multiclass/binary\n",
    "    if n_classes == 2:\n",
    "        Y_test = np.zeros((len(y_true), 2))\n",
    "        Y_test[np.arange(len(y_true)), y_true] = 1\n",
    "    else:\n",
    "        Y_test = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Plot diagonal \"chance\" line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Chance (AUC = 0.50)')\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        # Calculate False Positive Rate and True Positive Rate\n",
    "        fpr, tpr, _ = roc_curve(Y_test[:, i], Y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right', fontsize='small')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "plot_roc_curve(model, test_loader, DEVICE, manager.encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01109026-8ec8-4e59-8303-dd08d544bd43",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add30226-2f50-462b-91f6-4fe39310bf4d",
   "metadata": {},
   "source": [
    "Let's try to explain how our model works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101c714-ec6b-4f7f-9cf6-b1b40859e8af",
   "metadata": {},
   "source": [
    "### GradCAM for CNN branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063829ee-bc35-4174-892a-a165428f4580",
   "metadata": {},
   "source": [
    "We start with explaining how CNN branch works with Grad-CAM,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ee3b7-b833-4102-bc4c-13fe3737d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "\n",
    "class CNN_GradCAM:\n",
    "    def __init__(self, model, target_layer=None):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.features = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # Automatically find the last Conv2d layer if not specified\n",
    "        if target_layer is None:\n",
    "            target_layer = self._find_last_conv_layer()\n",
    "            print(f\"Automatically targeted layer: {target_layer}\")\n",
    "\n",
    "        # Register hooks\n",
    "        self._register_hooks(target_layer)\n",
    "\n",
    "    def _find_last_conv_layer(self):\n",
    "        # Heuristic to find the last Conv2d layer (usually part of the CNN block)\n",
    "        last_conv = None\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, torch.nn.Conv2d):\n",
    "                last_conv = module\n",
    "        return last_conv\n",
    "\n",
    "    def _register_hooks(self, layer):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.features = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        # 1. Forward Pass\n",
    "        self.model.zero_grad()\n",
    "        # Conformer specific: Unpack the tuple\n",
    "        out_cnn, out_trans = self.model(input_image)\n",
    "\n",
    "        # 2. Select Target\n",
    "        if target_class is None:\n",
    "            # Determine class based on combined logic\n",
    "            final_pred = (out_cnn + out_trans).argmax(dim=1)\n",
    "            target_class = final_pred.item()\n",
    "\n",
    "        # 3. Backward Pass on CNN Output ONLY\n",
    "        score = out_cnn[0, target_class]\n",
    "        score.backward()\n",
    "\n",
    "        # 4. Generate Grad-CAM weights\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "        # Weighted combination of feature maps\n",
    "        cam = torch.sum(weights * self.features, dim=1, keepdim=True)\n",
    "\n",
    "        # Apply ReLU\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-7)\n",
    "\n",
    "        return cam.cpu().detach().numpy()[0, 0], target_class\n",
    "\n",
    "def visualize_cam(original_img_tensor, cam_mask, class_name):\n",
    "    \"\"\"\n",
    "    Visualizes Grad-CAM using PIL and Matplotlib (No OpenCV).\n",
    "    \"\"\"\n",
    "    # 1. Convert tensor to numpy image (H, W, C)\n",
    "    img = original_img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # 2. Denormalize (assuming ImageNet means/stds)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # 3. Resize CAM to match Original Image using PIL\n",
    "    # Scale CAM to 0-255 uint8 for PIL processing\n",
    "    cam_uint8 = np.uint8(255 * cam_mask)\n",
    "    cam_pil = Image.fromarray(cam_uint8, 'L') # 'L' mode for grayscale\n",
    "\n",
    "    # Upsample using Bicubic interpolation\n",
    "    original_h, original_w = img.shape[:2]\n",
    "    cam_pil_resized = cam_pil.resize((original_w, original_h), resample=Image.BICUBIC)\n",
    "\n",
    "    # Convert back to numpy float 0-1\n",
    "    cam_resized = np.array(cam_pil_resized) / 255.0\n",
    "\n",
    "    # 4. Apply Colormap (Jet) using Matplotlib\n",
    "    # cm.jet returns (H, W, 4) RGBA, we only need RGB\n",
    "    heatmap = cm.jet(cam_resized)[..., :3]\n",
    "\n",
    "    # 5. Superimpose\n",
    "    alpha = 0.5\n",
    "    overlay = (1 - alpha) * img + alpha * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # 6. Plotting\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(overlay)\n",
    "    ax[1].set_title(f\"CNN Grad-CAM: {class_name}\")\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Example Usage Logic ---\n",
    "# (Assumes 'model', 'test_loader', 'DEVICE', and 'class_names' are defined from previous cells)\n",
    "\n",
    "# 1. Instantiate the CAM object\n",
    "# We let it auto-find the last conv layer in the CNN branch\n",
    "grad_cam = CNN_GradCAM(model)\n",
    "\n",
    "# 2. Get a sample image\n",
    "img_tensor, label = next(iter(test_loader))\n",
    "input_image = img_tensor[0].unsqueeze(0).to(DEVICE)\n",
    "true_class_name = manager.encoder.classes_[label[0].item()]\n",
    "\n",
    "# 3. Generate CAM\n",
    "mask, pred_class_idx = grad_cam.generate_cam(input_image)\n",
    "pred_class_name = manager.encoder.classes_[pred_class_idx]\n",
    "\n",
    "# 4. Visualize\n",
    "print(f\"True Class: {true_class_name} | Predicted Class: {pred_class_name}\")\n",
    "visualize_cam(input_image[0], mask, pred_class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef4151-a4ff-4fd0-a966-2b24d2f7a538",
   "metadata": {},
   "source": [
    "### Attention Rollout for Transformer branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c01e9-ec81-42a6-9b8a-1fa4f7a020db",
   "metadata": {},
   "source": [
    ".. the we explain transformer with Attention Rollout,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b74fd1-c0b0-4eaa-9daf-eb095b3e81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "\n",
    "class ConformerRollout:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.attentions = []\n",
    "        self.hooks = []\n",
    "\n",
    "    def _get_attn_hook(self):\n",
    "        def hook(module, input, output):\n",
    "            # The input to the dropout layer in a standard ViT Attention block\n",
    "            # is the attention matrix (Batch, Heads, Tokens, Tokens)\n",
    "            # We capture input[0] because input is a tuple\n",
    "            self.attentions.append(input[0].detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"\n",
    "        Recursively finds all 'attn_drop' modules in the model (standard in timm/Conformer)\n",
    "        and registers a forward hook to capture attention weights.\n",
    "        \"\"\"\n",
    "        self.attentions = []\n",
    "        self.hooks = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            # 'attn_drop' is the standard name in the Conformer source code for\n",
    "            # the dropout layer applied to the Softmax(Q@K) matrix.\n",
    "            if \"attn_drop\" in name:\n",
    "                self.hooks.append(module.register_forward_hook(self._get_attn_hook()))\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    def generate_rollout(self, input_tensor, discard_ratio=0.9):\n",
    "        \"\"\"\n",
    "        Computes the Attention Rollout mask.\n",
    "        \"\"\"\n",
    "        self.register_hooks()\n",
    "\n",
    "        # 1. Forward Pass\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            # Handle potential tuple output from Conformer\n",
    "            _ = self.model(input_tensor)\n",
    "\n",
    "        self.remove_hooks()\n",
    "\n",
    "        if not self.attentions:\n",
    "            print(\"Error: No attention layers captured. Check layer names.\")\n",
    "            return None\n",
    "\n",
    "        # 2. Compute Rollout\n",
    "        # Start with Identity Matrix to represent residual connections\n",
    "        # Shape: (Tokens, Tokens) matching the last layer\n",
    "        num_tokens = self.attentions[0].size(-1)\n",
    "        result = torch.eye(num_tokens)\n",
    "\n",
    "        # Iterate from input layer to output layer\n",
    "        for attn in self.attentions:\n",
    "            # attn shape: (1, Heads, Tokens, Tokens)\n",
    "            # We average attention weights across all heads\n",
    "            attn_fused = attn.mean(axis=1)[0] # Shape: (Tokens, Tokens)\n",
    "\n",
    "            # To account for Residual Connections in the Transformer Block (x + Attention(x)),\n",
    "            # we add the Identity matrix and re-normalize.\n",
    "            # Formula: A_hat = 0.5 * A + 0.5 * I\n",
    "            attn_fused = attn_fused + torch.eye(attn_fused.size(0))\n",
    "            attn_fused = attn_fused / attn_fused.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # Recursive Multiplication: Rollout_l = A_l * Rollout_{l-1}\n",
    "            result = torch.matmul(attn_fused, result)\n",
    "\n",
    "        # 3. Extract Class Token Attention\n",
    "        # The first token (index 0) is the CLS token. We want to see what pixels\n",
    "        # it attended to.\n",
    "        mask = result[0, 1:] # Drop index 0 (self-attention to CLS)\n",
    "\n",
    "        # Reshape to 2D map\n",
    "        # For 224x224 image and patch size 16, we expect 14x14 = 196 patches\n",
    "        width = int(np.sqrt(mask.size(0)))\n",
    "        mask = mask.reshape(width, width).numpy()\n",
    "\n",
    "        # Normalize for visualization (0 to 1)\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "        return mask\n",
    "\n",
    "def visualize_rollout(original_img_tensor, mask, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Visualizes Attention Rollout using PIL and Matplotlib (No OpenCV).\n",
    "    \"\"\"\n",
    "    # 1. Convert tensor to numpy image (H, W, C)\n",
    "    img = original_img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # 2. Denormalize (assuming ImageNet normalization used in the notebook)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # 3. Resize mask to match image dimensions using PIL\n",
    "    # Scale mask to 0-255 uint8\n",
    "    mask_uint8 = np.uint8(255 * mask)\n",
    "    mask_pil = Image.fromarray(mask_uint8, mode='L')\n",
    "\n",
    "    # Resize using Bicubic interpolation for smooth heatmap\n",
    "    original_h, original_w = img.shape[:2]\n",
    "    mask_resized_pil = mask_pil.resize((original_w, original_h), resample=Image.BICUBIC)\n",
    "\n",
    "    # Convert back to float 0-1\n",
    "    mask_resized = np.array(mask_resized_pil) / 255.0\n",
    "\n",
    "    # 4. Apply Colormap (Jet) using Matplotlib\n",
    "    # cm.jet returns RGBA (H, W, 4), discard Alpha channel\n",
    "    heatmap = cm.jet(mask_resized)[..., :3]\n",
    "\n",
    "    # 5. Overlay\n",
    "    overlay = (1 - alpha) * img + alpha * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # 6. Plotting\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Original Input\")\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(overlay)\n",
    "    ax[1].set_title(\"Transformer Attention Rollout (Global)\")\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "img_tensor, label = next(iter(test_loader))\n",
    "input_image = img_tensor[0].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# 2. Instantiate and Run\n",
    "rollout_explainer = ConformerRollout(model)\n",
    "mask = rollout_explainer.generate_rollout(input_image)\n",
    "\n",
    "# 3. Visualize\n",
    "if mask is not None:\n",
    "    visualize_rollout(input_image[0], mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f074ba-1bfd-4426-9d66-d9d30db05364",
   "metadata": {},
   "source": [
    "### Integrated Gradient for whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd677d-36d7-4ead-93f3-00ba1a78718c",
   "metadata": {},
   "source": [
    ".. adn then integrated gradient for holistic model explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb79fac-aadc-41da-ba21-f6824ef0ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "class ConformerWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps the Conformer to fuse the two branches into a single output.\n",
    "    This ensures IG explains the actual decision mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, conformer_model):\n",
    "        super(ConformerWrapper, self).__init__()\n",
    "        self.model = conformer_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Forward pass through original model\n",
    "        out_cnn, out_trans = self.model(x)\n",
    "\n",
    "        # 2. Fuse outputs (Summation, as per notebook logic)\n",
    "        return out_cnn + out_trans\n",
    "\n",
    "def explain_with_ig(model, input_tensor, target_class_idx, class_names):\n",
    "    \"\"\"\n",
    "    Computes and visualizes Integrated Gradients.\n",
    "    \"\"\"\n",
    "    # 1. Wrap the model\n",
    "    wrapped_model = ConformerWrapper(model)\n",
    "    wrapped_model.eval()\n",
    "\n",
    "    # 2. Initialize IG\n",
    "    ig = IntegratedGradients(wrapped_model)\n",
    "\n",
    "    # 3. Compute Attributions\n",
    "    # We attribute the decision to the input pixels relative to a black baseline\n",
    "    print(\"Computing Integrated Gradients... (this may take a moment)\")\n",
    "    attributions, delta = ig.attribute(\n",
    "        input_tensor,\n",
    "        target=target_class_idx,\n",
    "        baselines=torch.zeros_like(input_tensor),\n",
    "        n_steps=50, # Higher steps = more accurate approximation of the integral\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    print(f\"Convergence Delta: {delta.item()} (Lower is better)\")\n",
    "\n",
    "    # 4. Prepare for Visualization\n",
    "    # Transpose to (H, W, C) for Matplotlib\n",
    "    attr_np = attributions[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    img_np = input_tensor[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Denormalize Image (Restoring original colors)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_np = std * img_np + mean\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    # 5. Visualize\n",
    "    # 'blended_heat_map' overlays the attribution on the image\n",
    "    viz.visualize_image_attr_multiple(\n",
    "        attr_np,\n",
    "        img_np,\n",
    "        methods=[\"original_image\", \"blended_heat_map\"],\n",
    "        signs=[\"all\", \"positive\"],\n",
    "        show_colorbar=True,\n",
    "        titles=[f\"Original: {class_names[target_class_idx]}\", \"Integrated Gradients\"],\n",
    "        fig_size=(10, 5)\n",
    "    )\n",
    "\n",
    "# 1. Get a single sample\n",
    "img_tensor, label = next(iter(test_loader))\n",
    "input_image = img_tensor[0].unsqueeze(0).to(DEVICE)\n",
    "target_class = label[0].item()\n",
    "\n",
    "# 2. Get class names (if available from manager)\n",
    "try:\n",
    "    class_names = manager.encoder.classes_\n",
    "except:\n",
    "    class_names = [f\"Class {i}\" for i in range(7)]\n",
    "\n",
    "# 3. Run Explanation\n",
    "explain_with_ig(model, input_image, target_class, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb75ebf-f5b0-4ccb-8c78-710c9aafa73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
